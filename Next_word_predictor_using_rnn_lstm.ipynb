{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load the IMDB dataset\n",
        "from tensorflow.keras.datasets import imdb"
      ],
      "metadata": {
        "id": "HQG5Hk5jCkA7"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PT8vrEFq-ihW",
        "outputId": "53c51b39-3b74-48c6-a5b1-0a3b857631ad"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading IMDB dataset...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "\u001b[1m1641221/1641221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Preparing corpus...\n",
            "Creating sequences...\n",
            "Total sequences: 100000\n",
            "Building the model...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training the model...\n",
            "Epoch 1/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 6ms/step - accuracy: 0.1077 - loss: 6.2742\n",
            "Epoch 2/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 5ms/step - accuracy: 0.1402 - loss: 5.6065\n",
            "Epoch 3/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 5ms/step - accuracy: 0.1599 - loss: 5.2893\n",
            "Epoch 4/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 5ms/step - accuracy: 0.1726 - loss: 5.0505\n",
            "Epoch 5/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 5ms/step - accuracy: 0.1800 - loss: 4.8537\n",
            "Epoch 6/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5ms/step - accuracy: 0.1886 - loss: 4.6562\n",
            "Epoch 7/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5ms/step - accuracy: 0.1973 - loss: 4.4502\n",
            "Epoch 8/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5ms/step - accuracy: 0.2091 - loss: 4.2574\n",
            "Epoch 9/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5ms/step - accuracy: 0.2256 - loss: 4.0550\n",
            "Epoch 10/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 5ms/step - accuracy: 0.2391 - loss: 3.8885\n",
            "Epoch 11/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5ms/step - accuracy: 0.2616 - loss: 3.7187\n",
            "Epoch 12/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5ms/step - accuracy: 0.2810 - loss: 3.5504\n",
            "Epoch 13/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5ms/step - accuracy: 0.3024 - loss: 3.3944\n",
            "Epoch 14/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 5ms/step - accuracy: 0.3249 - loss: 3.2437\n",
            "Epoch 15/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5ms/step - accuracy: 0.3511 - loss: 3.0888\n",
            "Epoch 16/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 5ms/step - accuracy: 0.3700 - loss: 2.9664\n",
            "Epoch 17/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 5ms/step - accuracy: 0.3962 - loss: 2.8347\n",
            "Epoch 18/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5ms/step - accuracy: 0.4158 - loss: 2.7100\n",
            "Epoch 19/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5ms/step - accuracy: 0.4354 - loss: 2.5992\n",
            "Epoch 20/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5ms/step - accuracy: 0.4575 - loss: 2.4924\n",
            "Epoch 21/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 7ms/step - accuracy: 0.4781 - loss: 2.3845\n",
            "Epoch 22/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 9ms/step - accuracy: 0.4994 - loss: 2.2850\n",
            "Epoch 23/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 6ms/step - accuracy: 0.5172 - loss: 2.1901\n",
            "Epoch 24/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 5ms/step - accuracy: 0.5324 - loss: 2.1171\n",
            "Epoch 25/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5ms/step - accuracy: 0.5531 - loss: 2.0197\n",
            "Epoch 26/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 5ms/step - accuracy: 0.5672 - loss: 1.9493\n",
            "Epoch 27/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 5ms/step - accuracy: 0.5848 - loss: 1.8717\n",
            "Epoch 28/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 5ms/step - accuracy: 0.5955 - loss: 1.8125\n",
            "Epoch 29/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 5ms/step - accuracy: 0.6103 - loss: 1.7488\n",
            "Epoch 30/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5ms/step - accuracy: 0.6249 - loss: 1.6789\n",
            "Epoch 31/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 5ms/step - accuracy: 0.6324 - loss: 1.6448\n",
            "Epoch 32/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 5ms/step - accuracy: 0.6485 - loss: 1.5654\n",
            "Epoch 33/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 5ms/step - accuracy: 0.6601 - loss: 1.5215\n",
            "Epoch 34/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 5ms/step - accuracy: 0.6684 - loss: 1.4742\n",
            "Epoch 35/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 5ms/step - accuracy: 0.6760 - loss: 1.4470\n",
            "Epoch 36/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5ms/step - accuracy: 0.6862 - loss: 1.3860\n",
            "Epoch 37/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 5ms/step - accuracy: 0.6950 - loss: 1.3602\n",
            "Epoch 38/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 5ms/step - accuracy: 0.7052 - loss: 1.3130\n",
            "Epoch 39/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5ms/step - accuracy: 0.7105 - loss: 1.2829\n",
            "Epoch 40/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 5ms/step - accuracy: 0.7167 - loss: 1.2501\n",
            "Epoch 41/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.7262 - loss: 1.2149\n",
            "Epoch 42/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 5ms/step - accuracy: 0.7304 - loss: 1.1871\n",
            "Epoch 43/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5ms/step - accuracy: 0.7383 - loss: 1.1569\n",
            "Epoch 44/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5ms/step - accuracy: 0.7428 - loss: 1.1332\n",
            "Epoch 45/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 5ms/step - accuracy: 0.7502 - loss: 1.1040\n",
            "Epoch 46/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5ms/step - accuracy: 0.7567 - loss: 1.0739\n",
            "Epoch 47/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5ms/step - accuracy: 0.7572 - loss: 1.0612\n",
            "Epoch 48/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5ms/step - accuracy: 0.7644 - loss: 1.0306\n",
            "Epoch 49/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 5ms/step - accuracy: 0.7680 - loss: 1.0131\n",
            "Epoch 50/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5ms/step - accuracy: 0.7737 - loss: 0.9942\n",
            "Epoch 51/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 5ms/step - accuracy: 0.7769 - loss: 0.9739\n",
            "Epoch 52/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 5ms/step - accuracy: 0.7813 - loss: 0.9487\n",
            "Epoch 53/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 5ms/step - accuracy: 0.7847 - loss: 0.9421\n",
            "Epoch 54/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5ms/step - accuracy: 0.7898 - loss: 0.9174\n",
            "Epoch 55/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5ms/step - accuracy: 0.7932 - loss: 0.9006\n",
            "Epoch 56/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 5ms/step - accuracy: 0.7940 - loss: 0.8850\n",
            "Epoch 57/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 5ms/step - accuracy: 0.7999 - loss: 0.8670\n",
            "Epoch 58/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5ms/step - accuracy: 0.8013 - loss: 0.8551\n",
            "Epoch 59/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5ms/step - accuracy: 0.8062 - loss: 0.8410\n",
            "Epoch 60/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5ms/step - accuracy: 0.8099 - loss: 0.8257\n",
            "Epoch 61/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5ms/step - accuracy: 0.8108 - loss: 0.8160\n",
            "Epoch 62/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 5ms/step - accuracy: 0.8124 - loss: 0.8063\n",
            "Epoch 63/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5ms/step - accuracy: 0.8186 - loss: 0.7799\n",
            "Epoch 64/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 5ms/step - accuracy: 0.8193 - loss: 0.7778\n",
            "Epoch 65/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 5ms/step - accuracy: 0.8225 - loss: 0.7695\n",
            "Epoch 66/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 5ms/step - accuracy: 0.8214 - loss: 0.7606\n",
            "Epoch 67/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5ms/step - accuracy: 0.8251 - loss: 0.7449\n",
            "Epoch 68/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5ms/step - accuracy: 0.8272 - loss: 0.7377\n",
            "Epoch 69/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5ms/step - accuracy: 0.8304 - loss: 0.7259\n",
            "Epoch 70/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5ms/step - accuracy: 0.8329 - loss: 0.7137\n",
            "Epoch 71/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 5ms/step - accuracy: 0.8315 - loss: 0.7139\n",
            "Epoch 72/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 5ms/step - accuracy: 0.8334 - loss: 0.7026\n",
            "Epoch 73/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5ms/step - accuracy: 0.8364 - loss: 0.6910\n",
            "Epoch 74/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 5ms/step - accuracy: 0.8383 - loss: 0.6822\n",
            "Epoch 75/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 5ms/step - accuracy: 0.8403 - loss: 0.6713\n",
            "Epoch 76/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 5ms/step - accuracy: 0.8408 - loss: 0.6686\n",
            "Epoch 77/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 5ms/step - accuracy: 0.8393 - loss: 0.6697\n",
            "Epoch 78/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5ms/step - accuracy: 0.8446 - loss: 0.6510\n",
            "Epoch 79/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 5ms/step - accuracy: 0.8478 - loss: 0.6438\n",
            "Epoch 80/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5ms/step - accuracy: 0.8473 - loss: 0.6397\n",
            "Epoch 81/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5ms/step - accuracy: 0.8496 - loss: 0.6316\n",
            "Epoch 82/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 5ms/step - accuracy: 0.8490 - loss: 0.6264\n",
            "Epoch 83/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 5ms/step - accuracy: 0.8530 - loss: 0.6187\n",
            "Epoch 84/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5ms/step - accuracy: 0.8505 - loss: 0.6167\n",
            "Epoch 85/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5ms/step - accuracy: 0.8543 - loss: 0.6123\n",
            "Epoch 86/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 5ms/step - accuracy: 0.8544 - loss: 0.6029\n",
            "Epoch 87/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5ms/step - accuracy: 0.8555 - loss: 0.5984\n",
            "Epoch 88/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 5ms/step - accuracy: 0.8573 - loss: 0.5949\n",
            "Epoch 89/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5ms/step - accuracy: 0.8622 - loss: 0.5791\n",
            "Epoch 90/90\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5ms/step - accuracy: 0.8576 - loss: 0.5837\n",
            "Evaluating the model...\n",
            "Model accuracy: 87.42%\n",
            "\n",
            "Next word predictions:\n",
            "the movie was -> less\n",
            "i really love -> the\n",
            "this film is -> so\n",
            "it is a -> very\n",
            "the acting was -> going\n"
          ]
        }
      ],
      "source": [
        "# Parameters\n",
        "VOCAB_SIZE = 5000  # Limit the vocabulary size to the top 5000 words\n",
        "MAX_SEQUENCE_LEN = 5  # We'll use sequences of length 5\n",
        "EMBEDDING_DIM = 50\n",
        "EPOCHS = 90  # Number of training epochs\n",
        "NUM_REVIEWS = 1000  # Limit to first 1000 reviews\n",
        "MAX_SEQUENCES = 100000  # Limit the total number of sequences\n",
        "\n",
        "# Load the data\n",
        "print(\"Loading IMDB dataset...\")\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=VOCAB_SIZE)\n",
        "\n",
        "# Use only a subset of the data to limit memory usage\n",
        "X_train = X_train[:NUM_REVIEWS]\n",
        "\n",
        "# Decode the sequences back to words\n",
        "index_word = imdb.get_word_index()\n",
        "word_index = {k: (v + 3) for k, v in index_word.items()}\n",
        "word_index[\"<PAD>\"] = 0\n",
        "word_index[\"<START>\"] = 1\n",
        "word_index[\"<UNK>\"] = 2\n",
        "word_index[\"<UNUSED>\"] = 3\n",
        "index_word = {v: k for k, v in word_index.items()}\n",
        "\n",
        "def decode_review(text):\n",
        "    return ' '.join([index_word.get(i, '?') for i in text])\n",
        "\n",
        "# Prepare a corpus of reviews\n",
        "print(\"Preparing corpus...\")\n",
        "corpus = []\n",
        "for sequence in X_train:\n",
        "    decoded = decode_review(sequence)\n",
        "    corpus.append(decoded)\n",
        "\n",
        "# Tokenize the corpus\n",
        "tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "\n",
        "total_words = VOCAB_SIZE\n",
        "\n",
        "# Create sequences\n",
        "print(\"Creating sequences...\")\n",
        "input_sequences = []\n",
        "sequence_counter = 0\n",
        "\n",
        "for line in corpus:\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(2, len(token_list)):\n",
        "        n_gram_sequence = token_list[i - MAX_SEQUENCE_LEN:i + 1]\n",
        "        if len(n_gram_sequence) == MAX_SEQUENCE_LEN + 1:\n",
        "            input_sequences.append(n_gram_sequence)\n",
        "            sequence_counter += 1\n",
        "            if sequence_counter >= MAX_SEQUENCES:\n",
        "                break\n",
        "    if sequence_counter >= MAX_SEQUENCES:\n",
        "        break\n",
        "\n",
        "print(f\"Total sequences: {len(input_sequences)}\")\n",
        "\n",
        "# Convert to numpy arrays and split into features and labels\n",
        "input_sequences = np.array(input_sequences)\n",
        "X = input_sequences[:, :-1]\n",
        "y = input_sequences[:, -1]\n",
        "\n",
        "# One-hot encode the labels\n",
        "y = to_categorical(y, num_classes=total_words)\n",
        "\n",
        "# Build the model\n",
        "print(\"Building the model...\")\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LEN))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "print(\"Training the model...\")\n",
        "history = model.fit(X, y, epochs=EPOCHS, verbose=1)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Evaluating the model...\")\n",
        "loss, accuracy = model.evaluate(X, y, verbose=0)\n",
        "print(f\"Model accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Function to predict the next word\n",
        "def predict_next_word(model, tokenizer, text):\n",
        "    token_list = tokenizer.texts_to_sequences([text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=MAX_SEQUENCE_LEN, padding='pre')\n",
        "    predicted_probs = model.predict(token_list, verbose=0)\n",
        "    predicted = np.argmax(predicted_probs, axis=-1)\n",
        "    output_word = tokenizer.index_word.get(predicted[0], '')\n",
        "    return output_word\n",
        "\n",
        "# Test the model\n",
        "seed_texts = [\n",
        "    \"the movie was\",\n",
        "    \"i really love\",\n",
        "    \"this film is\",\n",
        "    \"it is a\",\n",
        "    \"the acting was\"\n",
        "]\n",
        "\n",
        "print(\"\\nNext word predictions:\")\n",
        "for seed_text in seed_texts:\n",
        "    next_word = predict_next_word(model, tokenizer, seed_text)\n",
        "    print(f\"{seed_text} -> {next_word}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "# Save the model\n",
        "model.save('next_word_model.h5')\n",
        "print(\"\\nModel saved as 'next_word_model.h5'\")\n",
        "\n",
        "# Save the tokenizer\n",
        "with open('tokenizer.pkl', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "print(\"Tokenizer saved as 'tokenizer.pkl'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rh2OtFjD5zA",
        "outputId": "fa8848d3-d077-4377-e7ed-1323dfe45f65"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model saved as 'next_word_model.h5'\n",
            "Tokenizer saved as 'tokenizer.pkl'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Loading the Model and Tokenizer for Testing and Demonstration\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import load_model\n",
        "import pickle  # For loading the tokenizer\n",
        "\n",
        "# Parameters\n",
        "VOCAB_SIZE = 5000  # Ensure this matches the value used during training\n",
        "MAX_SEQUENCE_LEN = 5  # Ensure this matches the value used during training\n",
        "\n",
        "# Load the saved model\n",
        "model = load_model('next_word_model.h5')\n",
        "print(\"Model loaded from 'next_word_model.h5'\")\n",
        "\n",
        "# Load the saved tokenizer\n",
        "with open('tokenizer.pkl', 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)\n",
        "print(\"Tokenizer loaded from 'tokenizer.pkl'\")\n",
        "\n",
        "# Function to predict the next word\n",
        "def predict_next_word(model, tokenizer, text):\n",
        "    token_list = tokenizer.texts_to_sequences([text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=MAX_SEQUENCE_LEN, padding='pre')\n",
        "    predicted_probs = model.predict(token_list, verbose=0)\n",
        "    predicted = np.argmax(predicted_probs, axis=-1)\n",
        "    output_word = tokenizer.index_word.get(predicted[0], '')\n",
        "    return output_word\n",
        "\n",
        "# Test the model with some seed texts\n",
        "seed_texts = [\n",
        "    \"the movie was\",\n",
        "    \"i really love\",\n",
        "    \"this film is\",\n",
        "    \"it is a\",\n",
        "    \"the acting was\"\n",
        "]\n",
        "\n",
        "print(\"\\nNext word predictions:\")\n",
        "for seed_text in seed_texts:\n",
        "    next_word = predict_next_word(model, tokenizer, seed_text)\n",
        "    print(f\"{seed_text} -> {next_word}\")\n",
        "\n",
        "# Interactive testing\n",
        "print(\"\\nType a seed text to predict the next word (or 'exit' to quit):\")\n",
        "while True:\n",
        "    seed_text = input(\"Enter seed text: \")\n",
        "    if seed_text.lower() == 'exit':\n",
        "        break\n",
        "    next_word = predict_next_word(model, tokenizer, seed_text)\n",
        "    print(f\"{seed_text} -> {next_word}\")"
      ],
      "metadata": {
        "id": "o5iPbYZnD6pD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "321905aa-400a-45b1-9881-ccd68763335b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded from 'next_word_model.h5'\n",
            "Tokenizer loaded from 'tokenizer.pkl'\n",
            "\n",
            "Next word predictions:\n",
            "the movie was -> less\n",
            "i really love -> the\n",
            "this film is -> so\n",
            "it is a -> very\n",
            "the acting was -> going\n",
            "\n",
            "Type a seed text to predict the next word (or 'exit' to quit):\n",
            "Enter seed text: The movie is\n",
            "The movie is -> a\n",
            "Enter seed text: exit\n"
          ]
        }
      ]
    }
  ]
}